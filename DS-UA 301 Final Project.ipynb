{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ray\n",
    "import psutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from torchvision import utils, transforms\n",
    "import torch.nn.functional as F\n",
    "from extras.anchors import get_offsets\n",
    "from extras.boxes import box_iou, nms\n",
    "from extras.util import *\n",
    "from extras.image_manip import ManipDetectionModel\n",
    "from torch.backends import cudnn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import HyperBandScheduler\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if device == 'cuda':\n",
    "    cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tp_75466_471421_31.84_62.29_640.0_251.24_airpl...</td>\n",
       "      <td>11.8400</td>\n",
       "      <td>66.35659</td>\n",
       "      <td>640.0000</td>\n",
       "      <td>368.3100</td>\n",
       "      <td>tamper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COCO_train2014_000000075466.jpg</td>\n",
       "      <td>11.8400</td>\n",
       "      <td>42.29000</td>\n",
       "      <td>660.0000</td>\n",
       "      <td>271.2400</td>\n",
       "      <td>authentic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tp_354517_564330_393.81_312.08_640.0_422.7_bed...</td>\n",
       "      <td>275.3575</td>\n",
       "      <td>447.75460</td>\n",
       "      <td>480.0000</td>\n",
       "      <td>640.0000</td>\n",
       "      <td>tamper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>COCO_train2014_000000354517.jpg</td>\n",
       "      <td>373.8100</td>\n",
       "      <td>292.08000</td>\n",
       "      <td>660.0000</td>\n",
       "      <td>442.7000</td>\n",
       "      <td>authentic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tp_251466_448046_38.24_19.17_219.78_317.53_per...</td>\n",
       "      <td>9.8750</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>191.7031</td>\n",
       "      <td>268.0703</td>\n",
       "      <td>tamper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21517</th>\n",
       "      <td>COCO_train2014_000000110230.jpg</td>\n",
       "      <td>110.5500</td>\n",
       "      <td>144.04000</td>\n",
       "      <td>349.8100</td>\n",
       "      <td>274.2300</td>\n",
       "      <td>authentic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21518</th>\n",
       "      <td>Tp_39138_135332_149.93_12.94_267.51_125.12_ref...</td>\n",
       "      <td>129.9300</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>287.5100</td>\n",
       "      <td>144.3380</td>\n",
       "      <td>tamper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21519</th>\n",
       "      <td>COCO_train2014_000000039138.jpg</td>\n",
       "      <td>129.9300</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>287.5100</td>\n",
       "      <td>145.1200</td>\n",
       "      <td>authentic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21520</th>\n",
       "      <td>Tp_22726_506843_1.01_123.64_640.0_400.33_airpl...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>60.59902</td>\n",
       "      <td>640.0000</td>\n",
       "      <td>280.9690</td>\n",
       "      <td>tamper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21521</th>\n",
       "      <td>COCO_train2014_000000022726.jpg</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>103.64000</td>\n",
       "      <td>660.0000</td>\n",
       "      <td>420.3300</td>\n",
       "      <td>authentic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21522 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0         1          2  \\\n",
       "0      Tp_75466_471421_31.84_62.29_640.0_251.24_airpl...   11.8400   66.35659   \n",
       "1                        COCO_train2014_000000075466.jpg   11.8400   42.29000   \n",
       "2      Tp_354517_564330_393.81_312.08_640.0_422.7_bed...  275.3575  447.75460   \n",
       "3                        COCO_train2014_000000354517.jpg  373.8100  292.08000   \n",
       "4      Tp_251466_448046_38.24_19.17_219.78_317.53_per...    9.8750    0.00000   \n",
       "...                                                  ...       ...        ...   \n",
       "21517                    COCO_train2014_000000110230.jpg  110.5500  144.04000   \n",
       "21518  Tp_39138_135332_149.93_12.94_267.51_125.12_ref...  129.9300    0.00000   \n",
       "21519                    COCO_train2014_000000039138.jpg  129.9300    0.00000   \n",
       "21520  Tp_22726_506843_1.01_123.64_640.0_400.33_airpl...    0.0000   60.59902   \n",
       "21521                    COCO_train2014_000000022726.jpg    0.0000  103.64000   \n",
       "\n",
       "              3         4          5  \n",
       "0      640.0000  368.3100     tamper  \n",
       "1      660.0000  271.2400  authentic  \n",
       "2      480.0000  640.0000     tamper  \n",
       "3      660.0000  442.7000  authentic  \n",
       "4      191.7031  268.0703     tamper  \n",
       "...         ...       ...        ...  \n",
       "21517  349.8100  274.2300  authentic  \n",
       "21518  287.5100  144.3380     tamper  \n",
       "21519  287.5100  145.1200  authentic  \n",
       "21520  640.0000  280.9690     tamper  \n",
       "21521  660.0000  420.3300  authentic  \n",
       "\n",
       "[21522 rows x 6 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file_info_frame = pd.read_csv('train_filter.txt', delimiter=\" \", header=None)\n",
    "train_file_info_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tp_513867_476061_362.16_302.48_418.79_343.21_p...</td>\n",
       "      <td>342.16000</td>\n",
       "      <td>274.50390</td>\n",
       "      <td>438.7900</td>\n",
       "      <td>354.1598</td>\n",
       "      <td>tamper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COCO_train2014_000000513867.jpg</td>\n",
       "      <td>342.16000</td>\n",
       "      <td>282.48000</td>\n",
       "      <td>438.7900</td>\n",
       "      <td>363.2100</td>\n",
       "      <td>authentic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tp_88719_197636_462.5_162.16_594.92_420.28_per...</td>\n",
       "      <td>442.50000</td>\n",
       "      <td>142.16000</td>\n",
       "      <td>614.9200</td>\n",
       "      <td>427.0000</td>\n",
       "      <td>tamper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>COCO_train2014_000000088719.jpg</td>\n",
       "      <td>442.50000</td>\n",
       "      <td>142.16000</td>\n",
       "      <td>614.9200</td>\n",
       "      <td>440.2800</td>\n",
       "      <td>authentic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tp_319765_216279_1.43_14.35_444.84_374.53_pers...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>613.1200</td>\n",
       "      <td>299.7271</td>\n",
       "      <td>tamper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2139</th>\n",
       "      <td>COCO_train2014_000000028764.jpg</td>\n",
       "      <td>89.82000</td>\n",
       "      <td>223.22000</td>\n",
       "      <td>149.3600</td>\n",
       "      <td>323.3700</td>\n",
       "      <td>authentic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2140</th>\n",
       "      <td>Tp_353589_167910_76.15_107.57_268.0_601.8_pers...</td>\n",
       "      <td>33.89992</td>\n",
       "      <td>83.95565</td>\n",
       "      <td>209.6937</td>\n",
       "      <td>601.5795</td>\n",
       "      <td>tamper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2141</th>\n",
       "      <td>COCO_train2014_000000353589.jpg</td>\n",
       "      <td>56.15000</td>\n",
       "      <td>87.57000</td>\n",
       "      <td>288.0000</td>\n",
       "      <td>621.8000</td>\n",
       "      <td>authentic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2142</th>\n",
       "      <td>Tp_15816_320271_150.65_90.04_388.62_341.44_per...</td>\n",
       "      <td>130.65000</td>\n",
       "      <td>81.21593</td>\n",
       "      <td>408.6200</td>\n",
       "      <td>403.8201</td>\n",
       "      <td>tamper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2143</th>\n",
       "      <td>COCO_train2014_000000015816.jpg</td>\n",
       "      <td>130.65000</td>\n",
       "      <td>70.04000</td>\n",
       "      <td>408.6200</td>\n",
       "      <td>361.4400</td>\n",
       "      <td>authentic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2144 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0          1          2  \\\n",
       "0     Tp_513867_476061_362.16_302.48_418.79_343.21_p...  342.16000  274.50390   \n",
       "1                       COCO_train2014_000000513867.jpg  342.16000  282.48000   \n",
       "2     Tp_88719_197636_462.5_162.16_594.92_420.28_per...  442.50000  142.16000   \n",
       "3                       COCO_train2014_000000088719.jpg  442.50000  142.16000   \n",
       "4     Tp_319765_216279_1.43_14.35_444.84_374.53_pers...    0.00000    0.00000   \n",
       "...                                                 ...        ...        ...   \n",
       "2139                    COCO_train2014_000000028764.jpg   89.82000  223.22000   \n",
       "2140  Tp_353589_167910_76.15_107.57_268.0_601.8_pers...   33.89992   83.95565   \n",
       "2141                    COCO_train2014_000000353589.jpg   56.15000   87.57000   \n",
       "2142  Tp_15816_320271_150.65_90.04_388.62_341.44_per...  130.65000   81.21593   \n",
       "2143                    COCO_train2014_000000015816.jpg  130.65000   70.04000   \n",
       "\n",
       "             3         4          5  \n",
       "0     438.7900  354.1598     tamper  \n",
       "1     438.7900  363.2100  authentic  \n",
       "2     614.9200  427.0000     tamper  \n",
       "3     614.9200  440.2800  authentic  \n",
       "4     613.1200  299.7271     tamper  \n",
       "...        ...       ...        ...  \n",
       "2139  149.3600  323.3700  authentic  \n",
       "2140  209.6937  601.5795     tamper  \n",
       "2141  288.0000  621.8000  authentic  \n",
       "2142  408.6200  403.8201     tamper  \n",
       "2143  408.6200  361.4400  authentic  \n",
       "\n",
       "[2144 rows x 6 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_file_info_frame = pd.read_csv(\n",
    "    'test_filter.txt', delimiter=\" \", header=None)\n",
    "test_file_info_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_DIR = \"/scratch/hz2212/Final Project/train2014\"\n",
    "SYNTHETIC_DIR = \"/scratch/hz2212/Final Project/coco_synthetic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(filename):\n",
    "    imdir = SYNTHETIC_DIR if filename[:2] == \"Tp\" else COCO_DIR\n",
    "    return io.imread(os.path.join(imdir, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageManipDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, txt_file, transform=None, test_mode=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.file_info_frame = pd.read_csv(txt_file, delimiter=\" \", header=None)\n",
    "        if test_mode:\n",
    "            self.file_info_frame = pd.read_csv(txt_file, delimiter=\" \", header=None).head(2048)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_info_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        image = get_image(self.file_info_frame.iloc[idx, 0])\n",
    "        bbox = self.file_info_frame.iloc[idx, 1:5].values\n",
    "        is_authentic = 1 if self.file_info_frame.iloc[idx, 5] == \"authentic\" else 0\n",
    "        sample = {'image': image, 'bbox': bbox.reshape(1, -1)}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        sample[\"authentic\"] = is_authentic\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, bboxs = sample['image'], sample['bbox']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        img = transform.resize(image, (new_h, new_w))\n",
    "\n",
    "        # h and w are swapped for landmarks because for images,\n",
    "        # x and y axes are axis 1 and 0 respectively\n",
    "        bboxs = (bboxs * [new_w / w, new_h / h, new_w / w, new_h / h]).astype(float)\n",
    "\n",
    "        return {'image': img, 'bbox': bboxs}\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, bbox = sample['image'], sample['bbox']\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C x H x W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return {'image': torch.from_numpy(image),\n",
    "                'bbox': torch.from_numpy(bbox)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_transform = transforms.Compose([\n",
    "    Rescale((128, 128)),\n",
    "    ToTensor()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_train = ImageManipDataset(txt_file='train_filter.txt', transform=coco_transform, test_mode=False)\n",
    "transformed_test = ImageManipDataset(txt_file='test_filter.txt', transform=coco_transform, test_mode=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(transformed_train, batch_size=256, shuffle=True, pin_memory=True, num_workers=8)\n",
    "test_loader = DataLoader(transformed_test, batch_size=256, shuffle=False, pin_memory=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gt_boxes():\n",
    "    \"\"\"\n",
    "    Generate 192 boxes where each box is represented by :\n",
    "    [top_left_x, top_left_y, bottom_right_x, bottom_right_y]\n",
    "\n",
    "    Each anchor position should generate 3 boxes according to the scales and ratios given.\n",
    "\n",
    "    Return this result as a numpy array of size [192,4]\n",
    "    \"\"\"\n",
    "    stride = 16 # The stride of the final feature map is 16 (the model compresses the image from 128 x 128 to 8 x 8)\n",
    "    map_sz = 128 # this is the length of height/width of the image\n",
    "\n",
    "    scales = torch.tensor([10,20,30,40,50,60,70,80,90,100])\n",
    "    ratios = torch.tensor([[1,1], [0.7, 1.4], [1.4, 0.7], [0.8, 1.2], [1.2, 0.8], [0.6, 1.8], [1.8, 0.6]]).view(1, 14)\n",
    "    \n",
    "    half_stride = int(stride / 2)\n",
    "    num_grids = int((map_sz / stride) ** 2)\n",
    "    boxes_size = (ratios.T * scales).T.reshape(-1, 2)\n",
    "    num_boxes = boxes_size.shape[0] * num_grids\n",
    "    gt_boxes = torch.zeros((num_boxes, 4))\n",
    "\n",
    "    for i in range(num_boxes):\n",
    "        grid_index = i // (scales.shape[0] * ratios.shape[1] // 2)\n",
    "        box_index = i % (scales.shape[0] * ratios.shape[1] // 2)\n",
    "        center_x = int(grid_index % (map_sz / stride) * stride + half_stride)\n",
    "        center_y = int(grid_index // (map_sz / stride) * stride + half_stride)\n",
    "        top_left_x = center_x - (boxes_size[box_index, 0] / 2)\n",
    "        top_left_y = center_y - (boxes_size[box_index, 1] / 2)\n",
    "        bottom_right_x = center_x + (boxes_size[box_index, 0] / 2)\n",
    "        bottom_right_y = center_y + (boxes_size[box_index, 1] / 2)\n",
    "        gt_boxes[i, :] = torch.tensor([top_left_x, top_left_y, \n",
    "                                       bottom_right_x, bottom_right_y])\n",
    "\n",
    "\n",
    "\n",
    "    return gt_boxes\n",
    "\n",
    "gt_boxes = get_gt_boxes().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bbox_gt(ex_boxes, gt_boxes, is_auth, image_size=128):\n",
    "    '''\n",
    "\n",
    "    INPUT:\n",
    "    ex_boxes: [Nx4]: Bounding boxes in the image. Here N is the number of bounding boxes the image has\n",
    "    gt_boxes: [192 x 4]: Anchor boxes of an image of size 128 x 128 with stride 16.\n",
    "    sz : 128\n",
    "    OUTPUT:\n",
    "    gt_classes: [192 x 1] : Class labels for each anchor: 1 is for foreground, 0 is for background and -1 is for a bad anchor. [where IOU is between 0.3 and 0.7]\n",
    "    gt_offsets: [192 x 4]: Offsets for anchor to best fit the bounding box object. 0 values for 0 and -1 class anchors.\n",
    "\n",
    "    '''\n",
    "    high_threshold = 0.7\n",
    "    low_threshold = 0.3\n",
    "\n",
    "    iou, ex_index = box_iou(gt_boxes, ex_boxes).max(\n",
    "        dim=1)  # max iou and the index of bounding box\n",
    "    # nearest bounding box to each anchor box\n",
    "    crsp_ex_boxes = ex_boxes[ex_index, :]\n",
    "\n",
    "    gt_classes = -1 * torch.ones(gt_boxes.shape[0], 1).long().to(device)\n",
    "    gt_classes[iou > high_threshold] = 2 if is_auth else 1\n",
    "    gt_classes[iou < low_threshold] = 0\n",
    "\n",
    "    gt_offsets = get_offsets(gt_boxes, crsp_ex_boxes)\n",
    "    no_object = ((gt_classes == 0) | (gt_classes == -1)\n",
    "                 ).nonzero(as_tuple=True)[0]\n",
    "    gt_offsets[no_object, :] = torch.zeros(1, 4).to(device)\n",
    "    return gt_classes, gt_offsets\n",
    "\n",
    "\n",
    "def get_targets(sample, target, is_auth):\n",
    "    '''\n",
    "    Input\n",
    "    target => Set of bounding boxes for each image.\n",
    "    Sample => Each image\n",
    "    Output:\n",
    "    Bounding box offsets and class labels for each anchor.\n",
    "    '''\n",
    "\n",
    "    batched_preds = []\n",
    "    batched_offsets = []\n",
    "    final_cls_targets = []\n",
    "    final_box_offsets = []\n",
    "    for s, t, a in zip(sample, target, is_auth):\n",
    "        bboxes = t.float().reshape(1, -1).to(device)\n",
    "        class_targets, box_offsets = get_bbox_gt(bboxes, gt_boxes, a, image_size=128)\n",
    "        final_cls_targets.append(class_targets)\n",
    "        final_box_offsets.append(box_offsets)\n",
    "\n",
    "    final_cls_targets = torch.stack(final_cls_targets, dim=0)\n",
    "    final_box_offsets = torch.stack(final_box_offsets, dim=0)\n",
    "\n",
    "    return final_cls_targets, final_box_offsets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_loss(out_pred, class_targets):\n",
    "    # return class loss\n",
    "    #     print(out_pred.shape)\n",
    "    #     print(class_targets.shape)\n",
    "\n",
    "    class_targets_copy = class_targets.to(device)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-1, size_average=True).to(device)\n",
    "    class_targets_copy = class_targets_copy.squeeze()\n",
    "    \n",
    "    keep_idx = torch.cartesian_prod(torch.arange(class_targets.shape[0]), torch.randperm(class_targets.shape[1])[:20])\n",
    "    keep_idx = torch.cat((keep_idx.to(device), torch.argwhere(class_targets_copy > 0)))\n",
    "\n",
    "    #     out_pred[class_targets_copy < 0] = 0\n",
    "    #     class_targets_copy[class_targets_copy < 0] = 0\n",
    "    modified_class_targets = (torch.ones_like(class_targets) * -1)\n",
    "    modified_class_targets[keep_idx[:, 0], keep_idx[:, 1]] = class_targets_copy[keep_idx[:, 0], keep_idx[:, 1]] \n",
    "    return criterion(out_pred, modified_class_targets.to(device))\n",
    "\n",
    "\n",
    "def bbox_loss(out_bbox, box_targets, class_targets):\n",
    "    # return bounding box offset loss\n",
    "    box_targets = box_targets.to(device)\n",
    "    class_targets_copy = class_targets.to(device)\n",
    "    criterion = nn.SmoothL1Loss().to(device)\n",
    "    class_targets_copy = class_targets_copy.repeat(1, 1, 4)\n",
    "\n",
    "    out_bbox[class_targets_copy < 1] = 0\n",
    "    box_targets[class_targets_copy < 1] = 0\n",
    "    return criterion(out_bbox, box_targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Training Function.\n",
    "def train_manip(config):\n",
    "    \n",
    "    epochs=10\n",
    "    \n",
    "    model = ManipDetectionModel(base=config['base'], pretrained=config['pretrained']).to(device)\n",
    "    \n",
    "#     if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#         model = nn.DataParallel(model)\n",
    "    \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=config['lr'], momentum=0.9)\n",
    "    train_loader = DataLoader(transformed_train, batch_size=256, shuffle=True, pin_memory=True, num_workers=4)\n",
    "    \n",
    "#     scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)\n",
    "    \n",
    "    model_filename = \"model_resnet\" + str(config['base']) + \"_lr\" + str(config['lr'])\n",
    "    if config['pretrained']:\n",
    "        model_filename += '_pretrained'\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        \n",
    "        total_loss = 0\n",
    "        b_loss = 0\n",
    "        c_loss = 0\n",
    "\n",
    "        with tqdm(train_loader) as tepoch:\n",
    "            for data_dict in tepoch:\n",
    "                ims = data_dict[\"image\"].float().to(device)\n",
    "                class_targets, box_targets = get_targets(data_dict[\"image\"].to(device), data_dict[\"bbox\"].to(device), data_dict[\"authentic\"].to(device))\n",
    "                out_pred, out_box = model(ims)\n",
    "\n",
    "                loss_cls = class_loss(out_pred, class_targets.squeeze(2))\n",
    "                loss_bbox = bbox_loss(out_box, box_targets, class_targets)\n",
    "\n",
    "                loss = loss_cls + loss_bbox\n",
    "\n",
    "                if loss.item() != 0:\n",
    "                    optimizer.zero_grad(set_to_none=True)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                c_loss += loss_cls.item()\n",
    "                b_loss += loss_bbox.item()\n",
    "\n",
    "        avg_c_loss = float(c_loss / len(train_loader))\n",
    "        avg_b_loss = float(b_loss / len(train_loader))\n",
    "\n",
    "        print('Trained Epoch: {} | Avg Classification Loss: {}, Bounding Loss: {}\\n'.format(\n",
    "            i, avg_c_loss, avg_b_loss))\n",
    "        \n",
    "#         scheduler.step()\n",
    "\n",
    "        torch.save(model.state_dict(), \"/scratch/hz2212/Final Project/models/\" + model_filename)\n",
    "        \n",
    "        tune.report(weighted_loss=avg_c_loss + 1000 * avg_b_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix the issue on colab/hpc\n",
    "ray._private.utils.get_system_memory = lambda: psutil.virtual_memory().total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-05-15 22:44:51 (running for 00:05:50.57)<br>Memory usage on this node: 57.1/377.3 GiB<br>Using HyperBand: num_stopped=0 total_brackets=4\n",
       "Round #0:\n",
       "  Bracket(Max Size (n)=3, Milestone (r)=10, completed=0.0%): {RUNNING: 3} \n",
       "  Bracket(Max Size (n)=5, Milestone (r)=3, completed=0.0%): {PENDING: 2, RUNNING: 3} \n",
       "  Bracket(Max Size (n)=9, Milestone (r)=1, completed=0.0%): {PENDING: 9} \n",
       "Round #1:\n",
       "  Bracket(Max Size (n)=3, Milestone (r)=10, completed=0.0%): {PENDING: 1} <br>Resources requested: 48.0/48 CPUs, 1.9800000000000002/2 GPUs, 0.0/239.48 GiB heap, 0.0/106.62 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /home/hz2212/ray_results/train_manip_2022-05-15_22-39-01<br>Number of trials: 18/18 (12 PENDING, 6 RUNNING)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 17/85 [05:17<28:42, 25.33s/it]\n",
      " 21%|██        | 18/85 [05:14<20:12, 18.09s/it]\n",
      " 25%|██▍       | 21/85 [05:36<22:32, 21.14s/it]\n"
     ]
    }
   ],
   "source": [
    "hyperband_scheduler = HyperBandScheduler(\n",
    "    time_attr='training_iteration',\n",
    "    metric='weighted_loss',\n",
    "    mode='min',\n",
    "    max_t=10,\n",
    "    reduction_factor=3\n",
    ")\n",
    "\n",
    "analysis = tune.run(\n",
    "    train_manip,\n",
    "    resources_per_trial={\n",
    "        \"gpu\": 0.33,\n",
    "        \"cpu\": 8\n",
    "    },\n",
    "    config={\n",
    "        \"base\": tune.grid_search([18, 34, 50]),\n",
    "        \"lr\": tune.grid_search([0.01, 0.1, 1.0]),\n",
    "        \"pretrained\": tune.grid_search([True, False])\n",
    "    },\n",
    "    verbose=1,\n",
    "    scheduler=hyperband_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ax1.plot(train_clf_losses, label=\"Classification Loss\")\n",
    "ax1.set_xticks(np.arange(0, epochs, 1))\n",
    "ax1.set_xticklabels(np.arange(1, epochs + 1, 1))\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.legend()\n",
    "ax2.plot(train_reg_losses, label=\"Regression Loss\", c=\"orange\")\n",
    "ax2.set_xticks(np.arange(0, epochs, 1))\n",
    "ax2.set_xticklabels(np.arange(1, epochs + 1, 1))\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"Loss\")\n",
    "ax2.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = iter(test_loader).next()\n",
    "visDet(data_dict['image'][:4], data_dict['bbox'][:4], data_dict[\"authentic\"][:4].reshape(-1, 1))\n",
    "print(data_dict[\"authentic\"][:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visPred(model, sample):\n",
    "    # visualize your model predictions on the sample image.\n",
    "    model.eval()\n",
    "    sample = sample.float().to(device)\n",
    "    out_pred, out_box = model(sample)\n",
    "    sample = sample.cpu()\n",
    "    out_pred = out_pred.cpu()\n",
    "    out_box = out_box.cpu()\n",
    "    gt_boxes = get_gt_boxes()\n",
    "    probs = F.softmax(out_pred, dim=1)\n",
    "    preds = torch.argmax(probs, axis=1)\n",
    "    boxes = []\n",
    "    select_preds = []\n",
    "    for i in range(sample.shape[0]):\n",
    "        curr_pred = preds[i, :]\n",
    "        curr_probs = probs[i, :]\n",
    "        \n",
    "        gt_width = gt_boxes[:, 2] - gt_boxes[:, 0]\n",
    "        gt_height = gt_boxes[:, 3] - gt_boxes[:, 1]\n",
    "        gt_center_x = gt_boxes[:, 0] + 0.5 * gt_width\n",
    "        gt_center_y = gt_boxes[:, 1] + 0.5 * gt_height\n",
    "\n",
    "        ex_width = gt_width / torch.exp(out_box[i, :, 2])\n",
    "        ex_height = gt_height / torch.exp(out_box[i, :, 3])\n",
    "        ex_center_x = gt_center_x - out_box[i, :, 0] * ex_width\n",
    "        ex_center_y = gt_center_y - out_box[i, :, 1] * ex_height\n",
    "        \n",
    "        left_top_x = (ex_center_x - ex_width / 2).unsqueeze(1)\n",
    "        left_top_y = (ex_center_y - ex_height / 2).unsqueeze(1)\n",
    "        right_bottom_x = (ex_center_x + ex_height / 2).unsqueeze(1)\n",
    "        right_bottom_y = (ex_center_y + ex_height / 2).unsqueeze(1)\n",
    "        \n",
    "        max_probs, _ = torch.max(curr_probs, axis=0)\n",
    "        \n",
    "        selected_boxes = (curr_pred != 0) & (max_probs > 0.7)\n",
    "        \n",
    "        ex_boxes = torch.cat((left_top_x, left_top_y, right_bottom_x, right_bottom_y), dim=1)\n",
    "        ex_boxes = ex_boxes[selected_boxes]\n",
    "        curr_probs = curr_probs[:, selected_boxes]\n",
    "        curr_pred = curr_pred[selected_boxes]\n",
    "        max_probs = max_probs[selected_boxes]\n",
    "        \n",
    "#         print(curr_probs.shape)\n",
    "#         print(torch.argmax(curr_probs, axis=0).shape)\n",
    "\n",
    "        select_result = nms(ex_boxes, max_probs, 0.3)\n",
    "        ex_boxes = ex_boxes[select_result, :]\n",
    "        curr_pred = curr_pred[select_result]\n",
    "        \n",
    "        boxes.append(ex_boxes.detach())\n",
    "        select_preds.append(curr_pred.detach() - 1) # convert to 1 for auth 0 for tampered\n",
    "    \n",
    "    visDet(sample, boxes, select_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ManipDetectionModel().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "visPred(model, data_dict['image'].to(device=device, dtype=torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
